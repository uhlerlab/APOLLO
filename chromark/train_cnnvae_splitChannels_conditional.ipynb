{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29492e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.filters import gaussian, threshold_otsu\n",
    "from skimage.transform import resize\n",
    "from skimage import io\n",
    "import pickle\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import model.model_cnnvae_conditional\n",
    "import model.optimizer as optimizer\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharedSizes=[1024]\n",
    "dSpecific_filter=[(200,16)]\n",
    "pID_type='randInit'\n",
    "pIDemb_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdOutSamples=['HV1','P22','P14','P27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c1fcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sourceDir='/data/xinyi/c2p/data/chromark'\n",
    "segDir=os.path.join(sourceDir,'nuclear_masks')\n",
    "imgDir=os.path.join(sourceDir,'raw_data')\n",
    "conditions=['controls','headneck','meningioma', 'glioma']\n",
    "\n",
    "outSize=128\n",
    "savename='pathCentered_'+str(outSize)\n",
    "\n",
    "imgsC_all=None\n",
    "imgsP_all=None\n",
    "imgNames_all=None\n",
    "proteinNames=None\n",
    "pID_all=None\n",
    "for condition_i in conditions:\n",
    "    print(condition_i)\n",
    "    segDir_i=os.path.join(segDir,condition_i)\n",
    "    imgDir_i=os.path.join(imgDir,condition_i)\n",
    "    for stain in os.listdir(segDir_i):\n",
    "        print(stain)\n",
    "        segDir_i_stain=os.path.join(segDir_i,stain)\n",
    "        imgDir_i_stain=os.path.join(imgDir_i,stain)\n",
    "        \n",
    "        segPID2name={}\n",
    "        for pID_dir in os.listdir(segDir_i_stain):\n",
    "            pID=pID_dir.split('_')\n",
    "            segPID2name[pID[0]]=pID_dir\n",
    "        imgPID2name={}\n",
    "        for pID_dir in os.listdir(imgDir_i_stain):\n",
    "            pID=pID_dir.split('_')\n",
    "            imgPID2name[pID[0]]=pID_dir\n",
    "        for pID in segPID2name.keys():\n",
    "            if condition_i=='meningioma' and stain=='dapi_gh2ax_lamin_cd3' and pID=='P33': #skipping incorrect images\n",
    "                continue\n",
    "            if pID in holdOutSamples:\n",
    "                print('hold out: '+pID)\n",
    "                continue\n",
    "            print(pID)\n",
    "            if pID not in imgPID2name:\n",
    "                print('img not found '+pID)\n",
    "                continue\n",
    "            imgDir_i_stain_p=os.path.join(imgDir_i_stain,imgPID2name[pID])\n",
    "            segDir_i_stain_p=os.path.join(segDir_i_stain,segPID2name[pID])\n",
    "            \n",
    "            with open(os.path.join(imgDir_i_stain_p,savename+'_imgNames'), 'rb') as output:\n",
    "                imgNames=pickle.load(output)\n",
    "            with open(os.path.join(imgDir_i_stain_p,savename+'_img'), 'rb') as output:\n",
    "                img=pickle.load(output)\n",
    "\n",
    "            imgP=np.zeros((img.shape[0],1,img.shape[2],img.shape[3]))\n",
    "            proteinNames_curr=np.array([])\n",
    "            stain_list=stain.split('_')\n",
    "            nImgPerStain=int(img.shape[0]/(len(stain_list)-1))\n",
    "            np.random.seed(3)\n",
    "            allIdx_all=np.arange(img.shape[0])\n",
    "            np.random.shuffle(allIdx_all)\n",
    "            for s in range(1,len(stain_list)):\n",
    "                s_start=(s-1)*nImgPerStain\n",
    "                if s==len(stain_list)-1:\n",
    "                    s_end=img.shape[0]\n",
    "                else:\n",
    "                    s_end=s*nImgPerStain\n",
    "                imgP[s_start:s_end]=img[allIdx_all[s_start:s_end],s].reshape(s_end-s_start,1,img.shape[2],img.shape[3])\n",
    "                proteinNames_curr=np.concatenate((proteinNames_curr,np.repeat(stain_list[s],s_end-s_start)))\n",
    "            \n",
    "            if pID_all is None:\n",
    "                pID_all=np.repeat(pID,img.shape[0])\n",
    "                imgsC_all=img[allIdx_all,[0]]\n",
    "                imgNames_all=imgNames[allIdx_all]\n",
    "                proteinNames=proteinNames_curr\n",
    "                imgsP_all=imgP\n",
    "            else:\n",
    "                pID_all=np.concatenate((pID_all,np.repeat(pID,img.shape[0])))\n",
    "                imgsC_all=np.concatenate((imgsC_all,img[allIdx_all,[0]]),axis=0)\n",
    "                imgNames_all=np.concatenate((imgNames_all,imgNames[allIdx_all]))\n",
    "                proteinNames=np.concatenate((proteinNames,proteinNames_curr))\n",
    "                imgsP_all=np.concatenate((imgsP_all,imgP),axis=0)\n",
    "imgsC_all=imgsC_all.reshape(imgsC_all.shape[0],1,imgsC_all.shape[1],imgsC_all.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef74fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nProt=np.unique(proteinNames).size\n",
    "pnames,revIdx,pCounts=np.unique(proteinNames,return_inverse=True,return_counts=True)\n",
    "plabels=torch.tensor(np.arange(pnames.size)[revIdx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49419076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adf298",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d185ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE settings\n",
    "seed=3\n",
    "epochs=5001\n",
    "saveFreq=100\n",
    "lr=0.00001 #initial learning rate\n",
    "weight_decay=0 #Weight for L2 loss on embedding matrix.\n",
    "\n",
    "batchsize=256\n",
    "kernel_size=4\n",
    "stride=2\n",
    "padding=1\n",
    "\n",
    "# fc_dim1=6000\n",
    "hidden1=64 #Number of channels in hidden layer 1\n",
    "hidden2=128 \n",
    "hidden3=256\n",
    "hidden4=256\n",
    "hidden5=96\n",
    "hidden5_xy=4\n",
    "fc_dim1=96*hidden5_xy*hidden5_xy\n",
    "fc_dim2=6000\n",
    "\n",
    "dropout=0.01\n",
    "kl_weight=0.0000001\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8146552",
   "metadata": {},
   "outputs": [],
   "source": [
    "printFreq=1\n",
    "def train(epoch,imgs,trainIdx,valIdx,latentp,matchSize_p,model_train,optimizer_train,model_trainShared,optimizer_trainShared,sharedOnly,pLabels):\n",
    "    t = time.time()\n",
    "    model_train.train()\n",
    "    model_trainShared.train()\n",
    "\n",
    "    loss_kl_train_all=0\n",
    "    loss_x_train_all=0\n",
    "    loss_x_trainShared_all=0\n",
    "    loss_match_train_all=0\n",
    "    loss_all=0\n",
    "    ntrainBatches=int(np.ceil(trainIdx.shape[0]/batchsize))\n",
    "    for i in range(ntrainBatches):\n",
    "#         if i%200==0:\n",
    "#         print(i)\n",
    "        trainIdx_i=trainIdx[i*batchsize:min((i+1)*batchsize,trainIdx.shape[0])]\n",
    "        trainInput=torch.tensor(imgs[trainIdx_i])\n",
    "        trainInput_ID=pLabels[trainIdx_i].cuda()\n",
    "#         print(trainInput.shape)\n",
    "\n",
    "        trainInput=trainInput.cuda().float()\n",
    "        optimizer_train.zero_grad()\n",
    "        optimizer_trainShared.zero_grad()\n",
    "\n",
    "        recon, z, mu, logvar = model_train(trainInput,trainInput_ID)\n",
    "        reconShared=model_trainShared(z[:,:matchSize_p],model_train.pIDemb(trainInput_ID))\n",
    "        \n",
    "        loss_kl_train=loss_kl(mu, logvar)\n",
    "        loss_x_train=loss_x(recon, trainInput)\n",
    "        if latentp is not None:\n",
    "            loss_match_train=loss_match(z[:,:matchSize_p],latentp[trainIdx_i,:matchSize_p])\n",
    "        else:\n",
    "            loss_match_train=0\n",
    "        loss_xShared_train=loss_x(reconShared,trainInput)\n",
    "        if sharedOnly:\n",
    "            loss=loss_kl_train*kl_weight+loss_match_train*match_weight+loss_xShared_train*sharedWeight\n",
    "        else:\n",
    "            loss=loss_kl_train*kl_weight+loss_x_train+loss_match_train*match_weight+loss_xShared_train*sharedWeight\n",
    "\n",
    "        loss_kl_train_all+=loss_kl_train.item()\n",
    "        loss_x_train_all+=loss_x_train.item()\n",
    "        if latentp is not None:\n",
    "            loss_match_train_all+=loss_match_train.item()\n",
    "        loss_x_trainShared_all+=loss_xShared_train.item()\n",
    "        loss_all+=loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_train.step()\n",
    "        optimizer_trainShared.step()\n",
    "\n",
    "\n",
    "    loss_kl_train_all=loss_kl_train_all/ntrainBatches\n",
    "    loss_x_train_all=loss_x_train_all/ntrainBatches\n",
    "    loss_match_train_all=loss_match_train_all/ntrainBatches\n",
    "    loss_x_trainShared_all=loss_x_trainShared_all/ntrainBatches\n",
    "    loss_all=loss_all/ntrainBatches\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_train.eval()\n",
    "        model_trainShared.eval()\n",
    "\n",
    "        loss_val_all=0\n",
    "        loss_x_val_all=0\n",
    "        loss_match_val_all=0\n",
    "        loss_x_valShared_all=0\n",
    "        nvalBatches=int(np.ceil(valIdx.shape[0]/batchsize))\n",
    "        for i in range(nvalBatches):\n",
    "            valIdx_i=valIdx[i*batchsize:min((i+1)*batchsize,valIdx.shape[0])]\n",
    "            valInput=torch.tensor(imgs[valIdx_i])\n",
    "            valInput=valInput.cuda().float()\n",
    "            valInput_ID=pLabels[valIdx_i].cuda()\n",
    "            recon,z, mu, logvar = model_train(valInput,valInput_ID)\n",
    "            reconShared=model_trainShared(z[:,:matchSize_p],model_train.pIDemb(valInput_ID))\n",
    "\n",
    "            loss_x_val=loss_x(recon, valInput).item()\n",
    "            loss_x_valShared=loss_x(reconShared,valInput).item()\n",
    "            if latentp is not None:\n",
    "                loss_match_val=loss_match(z[:,:matchSize_p],latentp[valIdx_i,:matchSize_p]).item()\n",
    "            else:\n",
    "                loss_match_val=0\n",
    "            if sharedOnly:\n",
    "                loss_val=loss_match_val*match_weight+loss_x_valShared    \n",
    "            else:\n",
    "                loss_val=loss_x_val+loss_match_val*match_weight+loss_x_valShared\n",
    "\n",
    "            loss_x_val_all+=loss_x_val\n",
    "            loss_x_valShared_all+=loss_x_valShared\n",
    "            if latentp is not None:\n",
    "                loss_match_val_all+=loss_match_val\n",
    "            loss_val_all+=loss_val\n",
    "\n",
    "        loss_x_val_all=loss_x_val_all/nvalBatches\n",
    "        loss_x_valShared_all=loss_x_valShared_all/nvalBatches\n",
    "        loss_match_val_all=loss_match_val_all/nvalBatches\n",
    "        loss_val_all=loss_val_all/nvalBatches\n",
    "\n",
    "        latent_curr=None\n",
    "        nplotBatches=int(np.ceil(imgs.shape[0]/batchsize))\n",
    "        for i in range(nplotBatches):\n",
    "            plotInput=torch.tensor(imgs[i*batchsize:min((i+1)*batchsize,imgs.shape[0])])\n",
    "            plotInput=plotInput.cuda().float()\n",
    "            plotInput_ID=pLabels[i*batchsize:min((i+1)*batchsize,imgs.shape[0])].cuda()\n",
    "            recon,z, mu, logvar = model_train(plotInput,plotInput_ID)\n",
    "            if latent_curr is None:\n",
    "                latent_curr=z.detach()\n",
    "            else:\n",
    "                latent_curr=torch.cat((latent_curr,z.detach()),0)\n",
    "    if epoch%printFreq==0:\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'loss_train: {:.4f}'.format(loss_all),\n",
    "              'loss_kl_train: {:.4f}'.format(loss_kl_train_all),\n",
    "              'loss_x_train: {:.4f}'.format(loss_x_train_all),\n",
    "              'loss_xShared_train: {:.4f}'.format(loss_x_trainShared_all),\n",
    "              'loss_match_train: {:.4f}'.format(loss_match_train_all),\n",
    "              'loss_x_val: {:.4f}'.format(loss_x_val_all),\n",
    "              'loss_xShared_val: {:.4f}'.format(loss_x_valShared_all),\n",
    "              'loss_match_val: {:.4f}'.format(loss_match_val_all),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "    return latent_curr,loss_all,loss_kl_train_all,loss_x_train_all,loss_x_trainShared_all,loss_match_train_all,loss_val_all,loss_x_val_all,loss_x_valShared_all,loss_match_val_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_weight=1\n",
    "sharedWeight=1\n",
    "name_train='splitChannels_conditional_bce'\n",
    "modelname_train='cnn_vae_pbmc_cvae'\n",
    "logsavepath_train=os.path.join('/data/xinyi/c2p/log/',modelname_train,name_train)\n",
    "modelsavepath_train=os.path.join('/data/xinyi/c2p/models/',modelname_train,name_train)\n",
    "plotsavepath_train=os.path.join('/data/xinyi/c2p/plots/',modelname_train,name_train)\n",
    "\n",
    "if not os.path.exists(os.path.join('/data/xinyi/c2p/log/',modelname_train)):\n",
    "    os.mkdir(os.path.join('/data/xinyi/c2p/log/',modelname_train))\n",
    "    os.mkdir(os.path.join('/data/xinyi/c2p/models/',modelname_train))\n",
    "    os.mkdir(os.path.join('/data/xinyi/c2p/plots/',modelname_train))\n",
    "if not os.path.exists(logsavepath_train):\n",
    "    os.mkdir(logsavepath_train)\n",
    "    os.mkdir(os.path.join(logsavepath_train,'dna'))\n",
    "    os.mkdir(os.path.join(logsavepath_train,'protein'))\n",
    "if not os.path.exists(modelsavepath_train):\n",
    "    os.mkdir(modelsavepath_train)\n",
    "    os.mkdir(os.path.join(modelsavepath_train,'dna'))\n",
    "    os.mkdir(os.path.join(modelsavepath_train,'protein'))\n",
    "if not os.path.exists(plotsavepath_train):\n",
    "    os.mkdir(plotsavepath_train)\n",
    "    os.mkdir(os.path.join(plotsavepath_train,'dna'))\n",
    "    os.mkdir(os.path.join(plotsavepath_train,'protein'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50408f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "logsavepath_p_dna=os.path.join(logsavepath_train,'dna')\n",
    "modelsavepath_p_dna=os.path.join(modelsavepath_train,'dna')\n",
    "plotsavepath_p_dna=os.path.join(plotsavepath_train,'dna')\n",
    "if not os.path.exists(logsavepath_p_dna):\n",
    "    os.mkdir(logsavepath_p_dna)\n",
    "if not os.path.exists(modelsavepath_p_dna):\n",
    "    os.mkdir(modelsavepath_p_dna)\n",
    "if not os.path.exists(plotsavepath_p_dna):\n",
    "    os.mkdir(plotsavepath_p_dna)\n",
    "\n",
    "logsavepath_p_protein=os.path.join(logsavepath_train,'protein')\n",
    "modelsavepath_p_protein=os.path.join(modelsavepath_train,'protein')\n",
    "plotsavepath_p_protein=os.path.join(plotsavepath_train,'protein')\n",
    "if not os.path.exists(logsavepath_p_protein):\n",
    "    os.mkdir(logsavepath_p_protein)\n",
    "if not os.path.exists(modelsavepath_p_protein):\n",
    "    os.mkdir(modelsavepath_p_protein)\n",
    "if not os.path.exists(plotsavepath_p_protein):\n",
    "    os.mkdir(plotsavepath_p_protein)\n",
    "\n",
    "#train-test split\n",
    "np.random.seed(3)\n",
    "pctVal=0.05\n",
    "pctTest=0.1\n",
    "\n",
    "allIdx_all=np.arange(proteinNames.size)\n",
    "np.random.shuffle(allIdx_all)\n",
    "valIdx_all=allIdx_all[:int(pctVal*proteinNames.size)]\n",
    "testIdx_all=allIdx_all[int(pctVal*proteinNames.size):(int(pctVal*proteinNames.size)+int(pctTest*proteinNames.size))]\n",
    "trainIdx_all=allIdx_all[(int(pctVal*proteinNames.size)+int(pctTest*proteinNames.size)):]\n",
    "\n",
    "\n",
    "\n",
    "for currLatentSize in sharedSizes:\n",
    "    for dSpecificSize,dfilterSize in dSpecific_filter:\n",
    "        latent_curr=None\n",
    "#         if os.path.exists(os.path.join(plotsavepath_p_dna,'loss_seed3_match'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg')):\n",
    "#             continue\n",
    "\n",
    "\n",
    "        print(currLatentSize)\n",
    "        print(dSpecificSize)\n",
    "        dna_cShared=hidden5-dfilterSize\n",
    "        p_cShared=dna_cShared\n",
    "\n",
    "        loss_match=torch.nn.MSELoss()\n",
    "        loss_kl=optimizer.optimizer_kl\n",
    "        loss_x=torch.nn.BCELoss()\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        if modelname_train=='cnn_vae_pbmc_cvae':\n",
    "            modelcnn_dna = model.model_cnnvae_conditional.CNN_VAE_split_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, dna_cShared, dna_cShared*hidden5_xy*hidden5_xy,(hidden5-dna_cShared)*hidden5_xy*hidden5_xy,currLatentSize,dSpecificSize,pnames.size,'randInit',pIDemb_size)\n",
    "            modelcnn_protein = model.model_cnnvae_conditional.CNN_VAE_split_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5,p_cShared,p_cShared*hidden5_xy*hidden5_xy, (hidden5-p_cShared)*hidden5_xy*hidden5_xy,currLatentSize,dSpecificSize,pnames.size,'randInit',pIDemb_size)\n",
    "            modelcnn_dnaShared=model.model_cnnvae_conditional.CNN_VAE_decode_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, fc_dim1,currLatentSize,pIDemb_size)\n",
    "            modelcnn_pShared=model.model_cnnvae_conditional.CNN_VAE_decode_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, fc_dim1,currLatentSize,pIDemb_size)\n",
    "        modelcnn_dna.cuda()\n",
    "        modelcnn_protein.cuda()\n",
    "        modelcnn_dnaShared.cuda()\n",
    "        modelcnn_pShared.cuda()\n",
    "\n",
    "        optimizer_dna = torch.optim.Adam(modelcnn_dna.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer_protein = torch.optim.Adam(modelcnn_protein.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer_dnaShared = torch.optim.Adam(modelcnn_dnaShared.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        optimizer_pShared = torch.optim.Adam(modelcnn_pShared.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        train_loss_dna=[np.inf]*(epochs)\n",
    "        train_loss_kl_dna=[np.inf]*(epochs)\n",
    "        train_loss_x_dna=[np.inf]*(epochs)\n",
    "        train_loss_xShared_dna=[np.inf]*(epochs)\n",
    "        train_loss_match_dna=[np.inf]*(epochs)\n",
    "        val_loss_dna=[np.inf]*(epochs)\n",
    "        val_loss_x_dna=[np.inf]*(epochs)\n",
    "        val_loss_xShared_dna=[np.inf]*(epochs)\n",
    "        val_loss_match_dna=[np.inf]*(epochs)\n",
    "\n",
    "        train_loss_protein=[np.inf]*(epochs)\n",
    "        train_loss_kl_protein=[np.inf]*(epochs)\n",
    "        train_loss_x_protein=[np.inf]*(epochs)\n",
    "        train_loss_xShared_protein=[np.inf]*(epochs)\n",
    "        train_loss_match_protein=[np.inf]*(epochs)\n",
    "        val_loss_protein=[np.inf]*(epochs)\n",
    "        val_loss_x_protein=[np.inf]*(epochs)\n",
    "        val_loss_xShared_protein=[np.inf]*(epochs)\n",
    "        val_loss_match_protein=[np.inf]*(epochs)\n",
    "\n",
    "        t_ep=time.time()\n",
    "\n",
    "        stateDict_train_dna={}\n",
    "        stateDict_train_protein={}\n",
    "        stateDict_train_dnaShared={}\n",
    "        stateDict_train_proteinShared={}\n",
    "        \n",
    "\n",
    "            \n",
    "        latent_curr=None\n",
    "        epCounts=0\n",
    "        for ep in range(epochs):\n",
    "            latent_curr,train_loss_dna[ep],train_loss_kl_dna[ep],train_loss_x_dna[ep],train_loss_xShared_dna[ep],train_loss_match_dna[ep],val_loss_dna[ep],val_loss_x_dna[ep],val_loss_xShared_dna[ep],val_loss_match_dna[ep]=train(ep,imgsC_all,trainIdx_all,valIdx_all,latent_curr,currLatentSize,modelcnn_dna,optimizer_dna,modelcnn_dnaShared,optimizer_dnaShared,False,plabels)\n",
    "            latent_curr,train_loss_protein[ep],train_loss_kl_protein[ep],train_loss_x_protein[ep],train_loss_xShared_protein[ep],train_loss_match_protein[ep],val_loss_protein[ep],val_loss_x_protein[ep],val_loss_xShared_protein[ep],val_loss_match_protein[ep]=train(ep,imgsP_all,trainIdx_all,valIdx_all,latent_curr,currLatentSize,modelcnn_protein,optimizer_protein,modelcnn_pShared,optimizer_pShared,False,plabels)\n",
    "\n",
    "            if ep>200 and (val_loss_x_dna[ep]>=val_loss_x_dna[ep-200] or val_loss_x_protein[ep]>=val_loss_x_protein[ep-200] or val_loss_match_dna[ep]>=val_loss_match_dna[ep-200]):\n",
    "                epCounts+=1\n",
    "            else:\n",
    "                epCounts=0\n",
    "\n",
    "            if epCounts>100:\n",
    "                break\n",
    "\n",
    "\n",
    "            if ep%saveFreq == (saveFreq-1):\n",
    "                stateDict_train_dna[ep]=modelcnn_dna.cpu().state_dict()\n",
    "                stateDict_train_protein[ep]=modelcnn_protein.cpu().state_dict()\n",
    "                stateDict_train_dnaShared[ep]=modelcnn_dnaShared.cpu().state_dict()\n",
    "                stateDict_train_proteinShared[ep]=modelcnn_pShared.cpu().state_dict()\n",
    "\n",
    "\n",
    "            modelcnn_dna.cuda()\n",
    "            modelcnn_protein.cuda()\n",
    "            modelcnn_dnaShared.cuda()\n",
    "            modelcnn_pShared.cuda()\n",
    "            torch.cuda.empty_cache()\n",
    "        print(' total time: {:.4f}s'.format(time.time() - t_ep))\n",
    "\n",
    "        with open(os.path.join(modelsavepath_p_dna,'stateDict_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(stateDict_train_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(modelsavepath_p_protein,'stateDict_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(stateDict_train_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(modelsavepath_p_dna,'stateDictShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(stateDict_train_dnaShared, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(modelsavepath_p_protein,'stateDictShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(stateDict_train_proteinShared, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(os.path.join(logsavepath_p_dna,'train_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'train_loss_kl_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_kl_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'train_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_x_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'train_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_xShared_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'train_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_match_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'val_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'val_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_x_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'val_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_xShared_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_dna,'val_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_match_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(os.path.join(logsavepath_p_protein,'train_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'train_loss_kl_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_kl_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'train_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_x_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'train_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_match_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'val_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'val_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_x_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'val_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_match_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'train_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(train_loss_xShared_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(logsavepath_p_protein,'val_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "            pickle.dump(val_loss_xShared_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        totalepoch=np.argmin(np.array(val_loss_x_dna)+np.array(val_loss_x_protein)+np.array(val_loss_match_protein))\n",
    "\n",
    "        print('loss_val_p: {:.4f}'.format(val_loss_x_protein[totalepoch]),\n",
    "              'loss_val_c: {:.4f}'.format(val_loss_x_dna[totalepoch]),\n",
    "              'loss_val_match: {:.4f}'.format(val_loss_match_dna[totalepoch]))\n",
    "\n",
    "        plt.plot(np.arange(epochs),train_loss_match_dna)\n",
    "        plt.plot(np.arange(epochs),val_loss_match_dna)\n",
    "        # plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "        plt.legend(['training match loss','validation match loss'],loc='upper right')\n",
    "        plt.savefig(os.path.join(plotsavepath_p_dna,'loss_seed3_match'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        plt.plot(np.arange(epochs),train_loss_x_dna)\n",
    "        plt.plot(np.arange(epochs),val_loss_x_dna)\n",
    "        # plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "        plt.legend(['training x recon loss','validation x recon loss','training kl loss'],loc='upper right')\n",
    "        plt.savefig(os.path.join(plotsavepath_p_dna,'loss_seed3_x'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.arange(epochs),train_loss_match_protein)\n",
    "        plt.plot(np.arange(epochs),val_loss_match_protein)\n",
    "        # plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "        plt.legend(['training match loss','validation match loss'],loc='upper right')\n",
    "        plt.savefig(os.path.join(plotsavepath_p_protein,'loss_seed3_match'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.arange(epochs),train_loss_x_protein)\n",
    "        plt.plot(np.arange(epochs),val_loss_x_protein)\n",
    "        # plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "        plt.legend(['training x recon loss','validation x recon loss','training kl loss'],loc='upper right')\n",
    "        plt.savefig(os.path.join(plotsavepath_p_protein,'loss_seed3_x'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.arange(epochs),train_loss_xShared_dna)\n",
    "        plt.plot(np.arange(epochs),val_loss_xShared_dna)\n",
    "        # plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "        plt.legend(['training shared recon','validation shared recon'],loc='upper right')\n",
    "        plt.savefig(os.path.join(plotsavepath_p_dna,'loss_seed3_xShared'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.arange(epochs),train_loss_xShared_protein)\n",
    "        plt.plot(np.arange(epochs),val_loss_xShared_protein)\n",
    "        # plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "        plt.legend(['training shared recon','validation shared recon'],loc='upper right')\n",
    "        plt.savefig(os.path.join(plotsavepath_p_protein,'loss_seed3_xShared'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateDict_train_dna[ep]=modelcnn_dna.cpu().state_dict()\n",
    "stateDict_train_protein[ep]=modelcnn_protein.cpu().state_dict()\n",
    "stateDict_train_dnaShared[ep]=modelcnn_dnaShared.cpu().state_dict()\n",
    "stateDict_train_proteinShared[ep]=modelcnn_pShared.cpu().state_dict()\n",
    "with open(os.path.join(modelsavepath_p_dna,'stateDict_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(stateDict_train_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(modelsavepath_p_protein,'stateDict_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(stateDict_train_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(modelsavepath_p_dna,'stateDictShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(stateDict_train_dnaShared, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(modelsavepath_p_protein,'stateDictShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(stateDict_train_proteinShared, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(logsavepath_p_dna,'train_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'train_loss_kl_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_kl_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'train_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_x_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'train_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_xShared_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'train_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_match_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'val_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'val_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_x_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'val_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_xShared_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_dna,'val_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_match_dna, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(logsavepath_p_protein,'train_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'train_loss_kl_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_kl_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'train_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_x_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'train_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_match_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'val_loss_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'val_loss_x_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_x_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'val_loss_match_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_match_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'train_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(train_loss_xShared_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(logsavepath_p_protein,'val_loss_xShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'wb') as output:\n",
    "    pickle.dump(val_loss_xShared_protein, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "totalepoch=np.argmin(np.array(val_loss_x_dna)+np.array(val_loss_x_protein)+np.array(val_loss_match_protein))\n",
    "\n",
    "print('loss_val_p: {:.4f}'.format(val_loss_x_protein[totalepoch]),\n",
    "      'loss_val_c: {:.4f}'.format(val_loss_x_dna[totalepoch]),\n",
    "      'loss_val_match: {:.4f}'.format(val_loss_match_dna[totalepoch]))\n",
    "\n",
    "plt.plot(np.arange(epochs),train_loss_match_dna)\n",
    "plt.plot(np.arange(epochs),val_loss_match_dna)\n",
    "# plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training match loss','validation match loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath_p_dna,'loss_seed3_match'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(np.arange(epochs),train_loss_x_dna)\n",
    "plt.plot(np.arange(epochs),val_loss_x_dna)\n",
    "# plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training x recon loss','validation x recon loss','training kl loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath_p_dna,'loss_seed3_x'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(epochs),train_loss_match_protein)\n",
    "plt.plot(np.arange(epochs),val_loss_match_protein)\n",
    "# plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training match loss','validation match loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath_p_protein,'loss_seed3_match'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(epochs),train_loss_x_protein)\n",
    "plt.plot(np.arange(epochs),val_loss_x_protein)\n",
    "# plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training x recon loss','validation x recon loss','training kl loss'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath_p_protein,'loss_seed3_x'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(epochs),train_loss_xShared_dna)\n",
    "plt.plot(np.arange(epochs),val_loss_xShared_dna)\n",
    "# plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training shared recon','validation shared recon'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath_p_dna,'loss_seed3_xShared'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(epochs),train_loss_xShared_protein)\n",
    "plt.plot(np.arange(epochs),val_loss_xShared_protein)\n",
    "# plt.plot(np.arange(epochs),train_loss_kl_ep)\n",
    "plt.legend(['training shared recon','validation shared recon'],loc='upper right')\n",
    "plt.savefig(os.path.join(plotsavepath_p_protein,'loss_seed3_xShared'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)+'.jpg'))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542294f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelcnn_dna.cuda()\n",
    "modelcnn_protein.cuda()\n",
    "modelcnn_dnaShared.cuda()\n",
    "modelcnn_pShared.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aefc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    modelcnn_dna.eval()\n",
    "    modelcnn_dnaShared.eval()\n",
    "    modelcnn_protein.eval()\n",
    "    modelcnn_pShared.eval()\n",
    "\n",
    "    loss_x_val_allC=0\n",
    "    loss_x_valShared_allC=0\n",
    "    loss_x_val_allP=0\n",
    "    loss_x_valShared_allP=0\n",
    "    nvalBatches=int(np.ceil(valIdx_all.shape[0]/batchsize))\n",
    "    for i in range(nvalBatches):\n",
    "        valIdx_i=valIdx_all[i*batchsize:min((i+1)*batchsize,valIdx_all.shape[0])]\n",
    "        valInputC=torch.tensor(imgsC_all[valIdx_i])\n",
    "        valInputC=valInputC.cuda().float()\n",
    "        valInputP=torch.tensor(imgsP_all[valIdx_i])\n",
    "        valInputP=valInputP.cuda().float()\n",
    "        valInput_ID=plabels[valIdx_i].cuda()\n",
    "        \n",
    "        reconC,z, mu, logvar = modelcnn_dna(valInputC,valInput_ID)\n",
    "        reconSharedC=modelcnn_dnaShared(z[:,:currLatentSize],modelcnn_dna.pIDemb(valInput_ID))\n",
    "\n",
    "        loss_x_val=loss_x(reconC, valInputC).item()\n",
    "        loss_x_valShared=loss_x(reconSharedC,valInputC).item()\n",
    "\n",
    "        loss_x_val_allC+=loss_x_val\n",
    "        loss_x_valShared_allC+=loss_x_valShared\n",
    "        \n",
    "        reconP,z, mu, logvar = modelcnn_protein(valInputP,valInput_ID)\n",
    "        reconSharedP=modelcnn_pShared(z[:,:currLatentSize],modelcnn_protein.pIDemb(valInput_ID))\n",
    "\n",
    "        loss_x_val=loss_x(reconP, valInputP).item()\n",
    "        loss_x_valShared=loss_x(reconSharedP,valInputP).item()\n",
    "\n",
    "        loss_x_val_allP+=loss_x_val\n",
    "        loss_x_valShared_allP+=loss_x_valShared\n",
    "        \n",
    "        for i in range(3):\n",
    "            print(i)\n",
    "            print(proteinNames[valIdx_i][i])\n",
    "            plt.imshow(valInputP[i][0].cpu().detach().numpy())\n",
    "            plt.show()\n",
    "            plt.imshow(reconSharedP[i][0].cpu().detach().numpy())\n",
    "            plt.show()\n",
    "            plt.imshow(reconP[i][0].cpu().detach().numpy())\n",
    "            plt.show()\n",
    "            \n",
    "            plt.imshow(valInputC[i][0].cpu().detach().numpy())\n",
    "            plt.show()\n",
    "            plt.imshow(reconSharedC[i][0].cpu().detach().numpy())\n",
    "            plt.show()\n",
    "            plt.imshow(reconC[i][0].cpu().detach().numpy())\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    loss_x_val_allC=loss_x_val_allC/nvalBatches\n",
    "    loss_x_valShared_allC=loss_x_valShared_allC/nvalBatches\n",
    "    \n",
    "    loss_x_val_allP=loss_x_val_allP/nvalBatches\n",
    "    loss_x_valShared_allP=loss_x_valShared_allP/nvalBatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b96006",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsC_val=None\n",
    "imgsP_val=None\n",
    "imgNames_val=None\n",
    "proteinNames_val=None\n",
    "pID_val=None\n",
    "imgsP_val_all=None\n",
    "imgsP_val_all_names=None\n",
    "for condition_i in conditions:\n",
    "    print(condition_i)\n",
    "    segDir_i=os.path.join(segDir,condition_i)\n",
    "    imgDir_i=os.path.join(imgDir,condition_i)\n",
    "    for stain in os.listdir(segDir_i):\n",
    "        print(stain)\n",
    "        segDir_i_stain=os.path.join(segDir_i,stain)\n",
    "        imgDir_i_stain=os.path.join(imgDir_i,stain)\n",
    "        \n",
    "        segPID2name={}\n",
    "        for pID_dir in os.listdir(segDir_i_stain):\n",
    "            pID=pID_dir.split('_')\n",
    "            segPID2name[pID[0]]=pID_dir\n",
    "        imgPID2name={}\n",
    "        for pID_dir in os.listdir(imgDir_i_stain):\n",
    "            pID=pID_dir.split('_')\n",
    "            imgPID2name[pID[0]]=pID_dir\n",
    "        for pID in segPID2name.keys():\n",
    "            if condition_i=='meningioma' and stain=='dapi_gh2ax_lamin_cd3' and pID=='P33': #skipping incorrect images\n",
    "                continue\n",
    "            if pID not in holdOutSamples:\n",
    "                continue\n",
    "            print(pID)\n",
    "            if pID not in imgPID2name:\n",
    "                print('img not found '+pID)\n",
    "                continue\n",
    "            imgDir_i_stain_p=os.path.join(imgDir_i_stain,imgPID2name[pID])\n",
    "            segDir_i_stain_p=os.path.join(segDir_i_stain,segPID2name[pID])\n",
    "            \n",
    "            with open(os.path.join(imgDir_i_stain_p,savename+'_imgNames'), 'rb') as output:\n",
    "                imgNames=pickle.load(output)\n",
    "            with open(os.path.join(imgDir_i_stain_p,savename+'_img'), 'rb') as output:\n",
    "                img=pickle.load(output)\n",
    "\n",
    "            imgP=np.zeros((img.shape[0],1,img.shape[2],img.shape[3]))\n",
    "            imgP_all=np.zeros((img.shape[0],3,img.shape[2],img.shape[3]))\n",
    "            proteinNames_val_curr=np.array([])\n",
    "            imgsP_val_all_names_curr=None\n",
    "            stain_list=stain.split('_')\n",
    "            nImgPerStain=int(img.shape[0]/(len(stain_list)-1))\n",
    "            np.random.seed(3)\n",
    "            allIdx_all=np.arange(img.shape[0])\n",
    "            np.random.shuffle(allIdx_all)\n",
    "            for s in range(1,len(stain_list)):\n",
    "                s_start=(s-1)*nImgPerStain\n",
    "                if s==len(stain_list)-1:\n",
    "                    s_end=img.shape[0]\n",
    "                else:\n",
    "                    s_end=s*nImgPerStain\n",
    "                imgP[s_start:s_end]=img[allIdx_all[s_start:s_end],s].reshape(s_end-s_start,1,img.shape[2],img.shape[3])\n",
    "                proteinNames_val_curr=np.concatenate((proteinNames_val_curr,np.repeat(stain_list[s],s_end-s_start)))\n",
    "                imgP_all[s_start:s_end,:img.shape[1]-1]=img[allIdx_all[s_start:s_end],1:].reshape(s_end-s_start,img.shape[1]-1,img.shape[2],img.shape[3])\n",
    "                if imgsP_val_all_names_curr is None:\n",
    "                    imgsP_val_all_names_curr=np.tile(stain_list[1:],(s_end-s_start,1))\n",
    "                else:\n",
    "                    imgsP_val_all_names_curr=np.concatenate((imgsP_val_all_names_curr,np.tile(stain_list[1:],(s_end-s_start,1))),axis=0)\n",
    "            if imgsP_val_all_names_curr.shape[1]==2:\n",
    "                imgsP_val_all_names_curr=np.hstack((imgsP_val_all_names_curr,np.repeat('None',imgsP_val_all_names_curr.shape[0]).reshape(-1,1)))\n",
    "            if pID_val is None:\n",
    "                pID_val=np.repeat(pID,img.shape[0])\n",
    "                imgsC_val=img[allIdx_all,[0]]\n",
    "                imgNames_val=imgNames[allIdx_all]\n",
    "                proteinNames_val=proteinNames_val_curr\n",
    "                imgsP_val=imgP\n",
    "                imgsP_val_all=imgP_all\n",
    "                imgsP_val_all_names=imgsP_val_all_names_curr\n",
    "            else:\n",
    "                pID_val=np.concatenate((pID_val,np.repeat(pID,img.shape[0])))\n",
    "                imgsC_val=np.concatenate((imgsC_val,img[allIdx_all,[0]]),axis=0)\n",
    "                imgNames_val=np.concatenate((imgNames_val,imgNames[allIdx_all]))\n",
    "                proteinNames_val=np.concatenate((proteinNames_val,proteinNames_val_curr))\n",
    "                imgsP_val=np.concatenate((imgsP_val,imgP),axis=0)\n",
    "                imgsP_val_all=np.concatenate((imgsP_val_all,imgP_all),axis=0)\n",
    "                imgsP_val_all_names=np.concatenate((imgsP_val_all_names,imgsP_val_all_names_curr),axis=0)\n",
    "imgsC_val=imgsC_val.reshape(imgsC_val.shape[0],1,imgsC_val.shape[1],imgsC_val.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "nProt_val=np.unique(proteinNames_val).size\n",
    "pnames_val,revIdx_val,pCounts_val=np.unique(proteinNames_val,return_inverse=True,return_counts=True)\n",
    "plabels_val=torch.tensor(np.arange(pnames_val.size)[revIdx_val]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc69a0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logsavepath_p_dna=os.path.join(logsavepath_train,'dna')\n",
    "modelsavepath_p_dna=os.path.join(modelsavepath_train,'dna')\n",
    "plotsavepath_p_dna=os.path.join(plotsavepath_train,'dna')\n",
    "\n",
    "logsavepath_p_protein=os.path.join(logsavepath_train,'protein')\n",
    "modelsavepath_p_protein=os.path.join(modelsavepath_train,'protein')\n",
    "plotsavepath_p_protein=os.path.join(plotsavepath_train,'protein')\n",
    "\n",
    "currLatentSize=sharedSizes[0]\n",
    "dSpecificSize,dfilterSize=dSpecific_filter[0]\n",
    "print(currLatentSize)\n",
    "print(dSpecificSize)\n",
    "dna_cShared=hidden5-dfilterSize\n",
    "p_cShared=dna_cShared\n",
    "\n",
    "loss_match=torch.nn.MSELoss()\n",
    "loss_kl=optimizer.optimizer_kl\n",
    "loss_x=torch.nn.BCEWithLogitsLoss()\n",
    "loss_mse=torch.nn.MSELoss()\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "if modelname_train=='cnn_vae_pbmc_cvae':\n",
    "    modelcnn_dna = model.model_cnnvae_conditional.CNN_VAE_split_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, dna_cShared, dna_cShared*hidden5_xy*hidden5_xy,(hidden5-dna_cShared)*hidden5_xy*hidden5_xy,currLatentSize,dSpecificSize,pnames.size,'randInit',pIDemb_size)\n",
    "    modelcnn_protein = model.model_cnnvae_conditional.CNN_VAE_split_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5,p_cShared,p_cShared*hidden5_xy*hidden5_xy, (hidden5-p_cShared)*hidden5_xy*hidden5_xy,currLatentSize,dSpecificSize,pnames.size,'randInit',pIDemb_size)\n",
    "    modelcnn_dnaShared=model.model_cnnvae_conditional.CNN_VAE_decode_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, fc_dim1,currLatentSize,pIDemb_size)\n",
    "    modelcnn_pShared=model.model_cnnvae_conditional.CNN_VAE_decode_pIDemb(kernel_size, stride, padding, 1, hidden1, hidden2, hidden3, hidden4, hidden5, fc_dim1,currLatentSize,pIDemb_size)\n",
    "\n",
    "\n",
    "ep=184\n",
    "\n",
    "with open(os.path.join(modelsavepath_p_dna,'stateDict_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'rb') as output:\n",
    "    stateDict_train_dna=pickle.load(output)\n",
    "with open(os.path.join(modelsavepath_p_protein,'stateDict_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'rb') as output:\n",
    "    stateDict_train_protein=pickle.load(output)\n",
    "with open(os.path.join(modelsavepath_p_dna,'stateDictShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'rb') as output:\n",
    "    stateDict_train_dnaShared=pickle.load(output)\n",
    "with open(os.path.join(modelsavepath_p_protein,'stateDictShared_'+str(currLatentSize)+'_'+str(dSpecificSize)+'_'+str(dfilterSize)), 'rb') as output:\n",
    "    stateDict_train_proteinShared=pickle.load(output)\n",
    "\n",
    "modelcnn_dna.load_state_dict(stateDict_train_dna[ep])\n",
    "modelcnn_protein.load_state_dict(stateDict_train_protein[ep])\n",
    "modelcnn_dnaShared.load_state_dict(stateDict_train_dnaShared[ep])\n",
    "modelcnn_pShared.load_state_dict(stateDict_train_proteinShared[ep])\n",
    "\n",
    "modelcnn_dna.cuda()\n",
    "modelcnn_protein.cuda()\n",
    "modelcnn_dnaShared.cuda()\n",
    "modelcnn_pShared.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a172194",
   "metadata": {},
   "outputs": [],
   "source": [
    "valIdx=np.arange(imgsC_val.shape[0])\n",
    "batchsize=328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2a66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valIdx=np.arange(imgsC_val.shape[0])\n",
    "batchsize=328\n",
    "\n",
    "with torch.no_grad():\n",
    "    modelcnn_dna.eval()\n",
    "    modelcnn_dnaShared.eval()\n",
    "    modelcnn_protein.eval()\n",
    "    modelcnn_pShared.eval()\n",
    "\n",
    "    loss_x_val_allC=0\n",
    "    loss_x_valShared_allC=0\n",
    "    loss_x_val_allP=0\n",
    "    loss_x_valShared_allP=0\n",
    "    nvalBatches=int(np.ceil(valIdx.shape[0]/batchsize))\n",
    "    for i in range(nvalBatches):\n",
    "        valIdx_i=valIdx[i*batchsize:min((i+1)*batchsize,valIdx.shape[0])]\n",
    "        valInputC=torch.tensor(imgsC_val[valIdx_i])\n",
    "        valInputC=valInputC.cuda().float()\n",
    "        valInputP=torch.tensor(imgsP_val[valIdx_i])\n",
    "        valInputP=valInputP.cuda().float()\n",
    "        valInput_ID=plabels[valIdx_i].cuda()\n",
    "        \n",
    "        reconC,z, mu, logvar = modelcnn_dna(valInputC,valInput_ID)\n",
    "        reconSharedC=modelcnn_dnaShared(z[:,:currLatentSize],modelcnn_dna.pIDemb(valInput_ID))\n",
    "\n",
    "        loss_x_val=loss_x(reconC, valInputC).item()\n",
    "        loss_x_valShared=loss_x(reconSharedC,valInputC).item()\n",
    "\n",
    "        loss_x_val_allC+=loss_x_val\n",
    "        loss_x_valShared_allC+=loss_x_valShared\n",
    "        \n",
    "        reconP,z, mu, logvar = modelcnn_protein(valInputP,valInput_ID)\n",
    "        reconSharedP=modelcnn_pShared(z[:,:currLatentSize],modelcnn_protein.pIDemb(valInput_ID))\n",
    "\n",
    "        loss_x_val=loss_x(reconP, valInputP).item()\n",
    "        loss_x_valShared=loss_x(reconSharedP,valInputP).item()\n",
    "\n",
    "        loss_x_val_allP+=loss_x_val\n",
    "        loss_x_valShared_allP+=loss_x_valShared\n",
    "        \n",
    "        for i in range(10):\n",
    "            print(i)\n",
    "            print(proteinNames_val[valIdx_i][i])\n",
    "            plt.imshow(percentileNorm(valInputP[i][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "            plt.imshow(percentileNorm(reconSharedP[i][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "            plt.imshow(percentileNorm(reconP[i][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "            \n",
    "            plt.imshow(modeSub(valInputC[i][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "            plt.imshow(modeSub(reconSharedC[i][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "            plt.imshow(modeSub(reconC[i][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    loss_x_val_allC=loss_x_val_allC/nvalBatches\n",
    "    loss_x_valShared_allC=loss_x_valShared_allC/nvalBatches\n",
    "    \n",
    "    loss_x_val_allP=loss_x_val_allP/nvalBatches\n",
    "    loss_x_valShared_allP=loss_x_valShared_allP/nvalBatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c62954",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_x_noReduction=torch.nn.L1Loss(reduction='sum')\n",
    "with torch.no_grad():\n",
    "    modelcnn_dna.eval()\n",
    "    modelcnn_dnaShared.eval()\n",
    "    modelcnn_protein.eval()\n",
    "    modelcnn_pShared.eval()\n",
    "\n",
    "    loss_x_val_allC=0\n",
    "    loss_x_valShared_allC=0\n",
    "    loss_x_val_allP=0\n",
    "    loss_x_valShared_allP=0\n",
    "    loss_shared={}\n",
    "    loss_shared['gh2ax']=0\n",
    "    loss_shared['cd16']=0\n",
    "    loss_shared['cd3']=0\n",
    "    loss_shared['cd4']=0\n",
    "    loss_shared['cd8']=0\n",
    "    loss_shared['lamin']=0\n",
    "    loss_full={}\n",
    "    loss_full['gh2ax']=0\n",
    "    loss_full['cd16']=0\n",
    "    loss_full['cd3']=0\n",
    "    loss_full['cd4']=0\n",
    "    loss_full['cd8']=0\n",
    "    loss_full['lamin']=0\n",
    "    nvalBatches=int(np.ceil(valIdx.shape[0]/batchsize))\n",
    "    for i in range(nvalBatches):\n",
    "        valIdx_i=valIdx[i*batchsize:min((i+1)*batchsize,valIdx.shape[0])]\n",
    "        valInputC=torch.tensor(imgsC_val[valIdx_i])\n",
    "        valInputC=valInputC.cuda().float()\n",
    "        valInputP=torch.tensor(imgsP_val[valIdx_i])\n",
    "        valInputP=valInputP.cuda().float()\n",
    "        valInput_ID=plabels[valIdx_i].cuda()\n",
    "        \n",
    "        reconC,z, mu, logvar = modelcnn_dna(valInputC,valInput_ID)\n",
    "        reconSharedC=modelcnn_dnaShared(z[:,:currLatentSize],modelcnn_dna.pIDemb(valInput_ID))\n",
    "\n",
    "        loss_x_val=loss_mse(reconC, valInputC).item()\n",
    "        loss_x_valShared=loss_mse(reconSharedC,valInputC).item()\n",
    "\n",
    "        loss_x_val_allC+=loss_x_val\n",
    "        loss_x_valShared_allC+=loss_x_valShared\n",
    "        \n",
    "        reconP,z, mu, logvar = modelcnn_protein(valInputP,valInput_ID)\n",
    "        reconSharedP=modelcnn_pShared(z[:,:currLatentSize],modelcnn_protein.pIDemb(valInput_ID))\n",
    "\n",
    "        loss_x_val=loss_mse(reconP, valInputP).item()\n",
    "        loss_x_valShared=loss_mse(reconSharedP,valInputP).item()\n",
    "\n",
    "        loss_x_val_allP+=loss_x_val\n",
    "        loss_x_valShared_allP+=loss_x_valShared\n",
    "        \n",
    "        for p in np.unique(proteinNames_val[valIdx_i]):\n",
    "            loss_full[p]+=loss_x_noReduction(modeSub_torch(reconP[proteinNames_val[valIdx_i]==p]), valInputP[proteinNames_val[valIdx_i]==p]).item()/(valInputP.shape[2]*valInputP.shape[3])\n",
    "            loss_shared[p]+=loss_x_noReduction(modeSub_torch(reconSharedP[proteinNames_val[valIdx_i]==p]),valInputP[proteinNames_val[valIdx_i]==p]).item()/(valInputP.shape[2]*valInputP.shape[3])\n",
    "\n",
    "    loss_x_val_allC=loss_x_val_allC/nvalBatches\n",
    "    loss_x_valShared_allC=loss_x_valShared_allC/nvalBatches\n",
    "    \n",
    "    loss_x_val_allP=loss_x_val_allP/nvalBatches\n",
    "    loss_x_valShared_allP=loss_x_valShared_allP/nvalBatches\n",
    "    \n",
    "    for p in np.unique(proteinNames_val):\n",
    "        print(p)\n",
    "        loss_shared[p]=loss_shared[p]/np.sum(proteinNames_val==p)\n",
    "        loss_full[p]=loss_full[p]/np.sum(proteinNames_val==p)\n",
    "        print('shared',loss_shared[p])\n",
    "        print('full',loss_full[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_x_val_allC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_x_valShared_allC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentileNorm(img_c):\n",
    "#     intensity,intCounts=np.unique(img_c,return_counts=True)\n",
    "#     modeint=intensity[np.argmax(intCounts)]\n",
    "    modeint=np.percentile(img_c,80)\n",
    "    img_c=img_c-modeint\n",
    "    img_c[img_c<0]=0\n",
    "    img_c=img_c/np.max(img_c)\n",
    "    print(modeint)\n",
    "    return img_c\n",
    "\n",
    "def modeSub(img_c):\n",
    "    intensity,intCounts=np.unique(img_c,return_counts=True)\n",
    "    modeint=intensity[np.argmax(intCounts)]\n",
    "#     modeint=np.percentile(img_c,75)\n",
    "    img_c=img_c-modeint\n",
    "    img_c[img_c<0]=0\n",
    "    img_c=img_c/np.max(img_c)\n",
    "    print(modeint)\n",
    "    return img_c\n",
    "def modeSub_torch(img_c):\n",
    "    intensity,intCounts=torch.unique(img_c,return_counts=True)\n",
    "    modeint=intensity[torch.argmax(intCounts)]\n",
    "#     modeint=np.percentile(img_c,75)\n",
    "    img_c=img_c-modeint\n",
    "    img_c[img_c<0]=0\n",
    "    img_c=img_c/torch.max(img_c)\n",
    "#     print(modeint)\n",
    "    return img_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edef2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting prediction of  all proteins\n",
    "plottingIdx=np.array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,  328,\n",
    "                      329,  330,  331,  332,  333,  334,  335,  336,  337,  656,  657,\n",
    "                      658,  659,  660,  661,  662,  663,  664,  665,  984,  985,  986,\n",
    "                      987,  988,  989,  990,  991,  992,  993, 1312, 1313, 1314, 1315,\n",
    "                      1316, 1317, 1318, 1319, 1320, 1321, 1640, 1641, 1642, 1643, 1644,\n",
    "                      1645, 1646, 1647, 1648, 1649, 1968, 1969, 1970, 1971, 1972, 1973,\n",
    "                      1974, 1975, 1976, 1977, 2296, 2297, 2298, 2299, 2300, 2301, 2302,\n",
    "                      2303, 2304, 2305, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631,\n",
    "                      2632, 2633, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960,\n",
    "                    2961, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289])\n",
    "with torch.no_grad():\n",
    "    modelcnn_dna.eval()\n",
    "    modelcnn_protein.eval()\n",
    "    modelcnn_dna.cuda()\n",
    "    modelcnn_protein.cuda()\n",
    "\n",
    "\n",
    "    for i in range(plottingIdx.size):\n",
    "        print(i)\n",
    "        print('input img',proteinNames_val[plottingIdx][i])\n",
    "        \n",
    "        \n",
    "        valtarget_protein=torch.tensor(imgsP_val[[plottingIdx[i]]]).cuda().float()\n",
    "        valtarget_dna=torch.tensor(imgsC_val[[plottingIdx[i]]]).cuda().float()\n",
    "        valInput_ID_orig=plabels_val[[plottingIdx[i]]].cuda()\n",
    "        valIdx_i=torch.tensor([plottingIdx[i]])\n",
    "        valAllProteins=imgsP_val_all[plottingIdx[i]]\n",
    "        valAllProteins_names=imgsP_val_all_names[plottingIdx[i]]\n",
    "        \n",
    "        plt.imshow(modeSub(valtarget_dna[0][0].cpu().detach().numpy()))\n",
    "        plt.show()\n",
    "#         plt.imshow(percentileNorm(valtarget_protein[0][0].cpu().detach().numpy()))\n",
    "#         plt.show()\n",
    "        for pidx in range(3):\n",
    "            if valAllProteins_names[pidx]=='None':\n",
    "                continue\n",
    "            print('True ',valAllProteins_names[pidx])\n",
    "            plt.imshow(percentileNorm(valAllProteins[pidx]))\n",
    "            plt.show()\n",
    "        \n",
    "        for pidx in range(pnames.size):\n",
    "            print(pnames[pidx])\n",
    "            valInput_ID=torch.tensor([pidx]).cuda()\n",
    "            \n",
    "            reconC,z_c, mu, logvar= modelcnn_dna(valtarget_dna,valInput_ID_orig)\n",
    "            reconP,z_p, mu, logvar = modelcnn_protein(valtarget_protein,valInput_ID_orig)\n",
    "\n",
    "            reconShared_protein=modelcnn_pShared(z_c[:,:currLatentSize],modelcnn_protein.pIDemb(valInput_ID))\n",
    "\n",
    "\n",
    "\n",
    "            plt.imshow(percentileNorm(reconShared_protein[0][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "            plt.imshow(modeSub(reconShared_protein[0][0].cpu().detach().numpy()))\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsC_val_allProt={}\n",
    "imgsP_val_allProt={}\n",
    "imgsP_val_allProt_input={}\n",
    "imgNames_val_allProt={}\n",
    "pID_val_allProt={}\n",
    "conditions_val_allProt={}\n",
    "proteinNames_val_allProt={}\n",
    "for condition_i in conditions:\n",
    "    print(condition_i)\n",
    "    segDir_i=os.path.join(segDir,condition_i)\n",
    "    imgDir_i=os.path.join(imgDir,condition_i)\n",
    "    for stain in os.listdir(segDir_i):\n",
    "        print(stain)\n",
    "        segDir_i_stain=os.path.join(segDir_i,stain)\n",
    "        imgDir_i_stain=os.path.join(imgDir_i,stain)\n",
    "        \n",
    "        segPID2name={}\n",
    "        for pID_dir in os.listdir(segDir_i_stain):\n",
    "            pID=pID_dir.split('_')\n",
    "            segPID2name[pID[0]]=pID_dir\n",
    "        imgPID2name={}\n",
    "        for pID_dir in os.listdir(imgDir_i_stain):\n",
    "            pID=pID_dir.split('_')\n",
    "            imgPID2name[pID[0]]=pID_dir\n",
    "        for pID in segPID2name.keys():\n",
    "            if condition_i=='meningioma' and stain=='dapi_gh2ax_lamin_cd3' and pID=='P33': #skipping incorrect images\n",
    "                continue\n",
    "            if pID not in holdOutSamples:\n",
    "                continue\n",
    "            print(pID)\n",
    "            if pID not in imgPID2name:\n",
    "                print('img not found '+pID)\n",
    "                continue\n",
    "            imgDir_i_stain_p=os.path.join(imgDir_i_stain,imgPID2name[pID])\n",
    "            segDir_i_stain_p=os.path.join(segDir_i_stain,segPID2name[pID])\n",
    "            \n",
    "            with open(os.path.join(imgDir_i_stain_p,savename+'_imgNames'), 'rb') as output:\n",
    "                imgNames=pickle.load(output)\n",
    "            with open(os.path.join(imgDir_i_stain_p,savename+'_img'), 'rb') as output:\n",
    "                img=pickle.load(output)\n",
    "                \n",
    "#             imgP=np.zeros((img.shape[0],1,img.shape[2],img.shape[3]))\n",
    "#             proteinNames_val_curr=np.array([])\n",
    "            stain_list=stain.split('_')\n",
    "            nImgPerStain=int(img.shape[0]/(len(stain_list)-1))\n",
    "            np.random.seed(3)\n",
    "            allIdx_all=np.arange(img.shape[0])\n",
    "            np.random.shuffle(allIdx_all)\n",
    "            for s in range(1,len(stain_list)):\n",
    "                s_start=(s-1)*nImgPerStain\n",
    "                if s==len(stain_list)-1:\n",
    "                    s_end=img.shape[0]\n",
    "                else:\n",
    "                    s_end=s*nImgPerStain\n",
    "                proteinNames_val_curr=np.repeat(stain_list[s],s_end-s_start)\n",
    "                imgP=img[allIdx_all[s_start:s_end],s].reshape(s_end-s_start,1,img.shape[2],img.shape[3])\n",
    "\n",
    "                for sother in range(1,len(stain_list)):\n",
    "                    if sother==s:\n",
    "                        continue\n",
    "                    if stain_list[sother] not in imgsP_val_allProt.keys():\n",
    "                        pID_val_allProt[stain_list[sother]]=np.repeat(pID,s_end-s_start)\n",
    "                        imgsC_val_allProt[stain_list[sother]]=img[allIdx_all[s_start:s_end],[0]].reshape(s_end-s_start,1,imgsC_val.shape[2],imgsC_val.shape[3])\n",
    "                        imgNames_val_allProt[stain_list[sother]]=imgNames[allIdx_all[s_start:s_end]]\n",
    "                        imgsP_val_allProt[stain_list[sother]]=img[allIdx_all[s_start:s_end],[sother]].reshape(s_end-s_start,1,imgsC_val.shape[2],imgsC_val.shape[3])\n",
    "                        conditions_val_allProt[stain_list[sother]]=np.repeat(condition_i,s_end-s_start)\n",
    "                        proteinNames_val_allProt[stain_list[sother]]=proteinNames_val_curr\n",
    "                        imgsP_val_allProt_input[stain_list[sother]]=imgP\n",
    "                    else:\n",
    "                        pID_val_allProt[stain_list[sother]]=np.concatenate((pID_val_allProt[stain_list[sother]],np.repeat(pID,s_end-s_start)))\n",
    "                        imgsC_val_allProt[stain_list[sother]]=np.concatenate((imgsC_val_allProt[stain_list[sother]],img[allIdx_all[s_start:s_end],[0]].reshape(s_end-s_start,1,imgsC_val.shape[2],imgsC_val.shape[3])),axis=0)\n",
    "                        imgNames_val_allProt[stain_list[sother]]=np.concatenate((imgNames_val_allProt[stain_list[sother]],imgNames[allIdx_all[s_start:s_end]]))\n",
    "                        imgsP_val_allProt[stain_list[sother]]=np.concatenate((imgsP_val_allProt[stain_list[sother]],img[allIdx_all[s_start:s_end],[sother]].reshape(s_end-s_start,1,imgsC_val.shape[2],imgsC_val.shape[3])),axis=0)\n",
    "                        conditions_val_allProt[stain_list[sother]]=np.concatenate((conditions_val_allProt[stain_list[sother]],np.repeat(condition_i,s_end-s_start)))\n",
    "                        proteinNames_val_allProt[stain_list[sother]]=np.concatenate((proteinNames_val_allProt[stain_list[sother]],proteinNames_val_curr))\n",
    "                        imgsP_val_allProt_input[stain_list[sother]]=np.concatenate((imgsP_val_allProt_input[stain_list[sother]],imgP),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14206fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction loss of all proteins - l1 + thresh\n",
    "loss_l1=torch.nn.L1Loss()\n",
    "with torch.no_grad():\n",
    "    modelcnn_dna.eval()\n",
    "    modelcnn_protein.eval()\n",
    "\n",
    "    \n",
    "\n",
    "    for pidx in range(pnames.size):\n",
    "        print(pnames[pidx])\n",
    "        \n",
    "        plabels_orig=torch.zeros(proteinNames_val_allProt[pnames[pidx]].size,dtype=int)\n",
    "        for pidx_label in range(pnames.size):\n",
    "            plabels_orig[proteinNames_val_allProt[pnames[pidx]]==pnames[pidx_label]]=pidx_label\n",
    "        \n",
    "        valInput_ID_single=torch.tensor([pidx]).cuda()\n",
    "        \n",
    "        valIdx_p=np.arange(imgsP_val_allProt[pnames[pidx]].shape[0])\n",
    "        loss_x_valShared_all_protein=0\n",
    "        loss_x_val_all_protein=0\n",
    "        nvalBatches=int(np.ceil(valIdx_p.shape[0]/batchsize))\n",
    "        for i in range(nvalBatches):\n",
    "            valIdx_i=valIdx_p[i*batchsize:min((i+1)*batchsize,valIdx_p.shape[0])]\n",
    "            valtarget_protein=torch.tensor(imgsP_val_allProt_input[pnames[pidx]][valIdx_i]).cuda().float()\n",
    "            valtarget_protein_pred=torch.tensor(imgsP_val_allProt[pnames[pidx]][valIdx_i]).cuda().float()\n",
    "            valtarget_dna=torch.tensor(imgsC_val_allProt[pnames[pidx]][valIdx_i]).cuda().float()\n",
    "            valInput_ID=torch.repeat_interleave(valInput_ID_single,valIdx_i.shape[0]).cuda()\n",
    "            valInput_ID_orig=plabels_orig[valIdx_i].cuda()\n",
    "            valIdx_i=torch.tensor(valIdx_i)\n",
    "\n",
    "            reconC,z_c, mu, logvar= modelcnn_dna(valtarget_dna,valInput_ID_orig)\n",
    "            reconP,z_p, mu, logvar= modelcnn_protein(valtarget_protein,valInput_ID_orig)\n",
    "\n",
    "            reconShared_protein=modelcnn_pShared(z_c[:,:currLatentSize],modelcnn_protein.pIDemb(valInput_ID))\n",
    "\n",
    "\n",
    "\n",
    "            loss_xShared_val_protein=loss_l1(modeSub_torch(reconShared_protein),valtarget_protein_pred)\n",
    "\n",
    "            loss_x_valShared_all_protein+=loss_xShared_val_protein.item()\n",
    "\n",
    "\n",
    "        loss_x_valShared_all_protein=loss_x_valShared_all_protein/nvalBatches\n",
    "        print(loss_x_valShared_all_protein)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562df86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
